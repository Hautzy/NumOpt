{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 1\n",
    "\n",
    "By Julian Hautzmayer k11904007\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook I tested four different minimization techniques:\n",
    "\n",
    "* steepest descent\n",
    "* newton method\n",
    "* quasi newton\n",
    "* conjugate gradient (linear and non-linear). \n",
    "\n",
    "For simplification all used code is extracted to different .py files:\n",
    "* main.py - provides the main logic\n",
    "* problems - definition of problem functions, gradients and hessians, I used 5 different explicit functions\n",
    "    * sphere function (modified to the power of 4)\n",
    "    * matyas function (modified to the power of 4)\n",
    "    * booth function\n",
    "    * himmelblau function\n",
    "    * rosenbrock function\n",
    "* methods - definition of the different minimization algorithms and methods to represent the results\n",
    "\n",
    "pls have a look at the implementation as here you will only see the result and conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main   # from this script I trigger all methods with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic objective functions\n",
    "\n",
    "First I use my implementations of steepest descent, newton method, quasi newton and linear conjugate gradient to solve 5 randomly generated quadratic objective functions.\n",
    "\n",
    "For every random generated example all four methods are applied. \n",
    "For every applied method we start from a random point and make a number of iterations. \n",
    "Here you can see how in every case the loss decreases and converges.\n",
    "After all iterations you can see which point was reached.\n",
    "\n",
    "At the end we want to see if our result really found a minimum. Therefore we plug in the steps we took in the gradient and see that again in every case the gradient converges against 0 proving that we indeed found a minimum in every case. After every method run with an example this can be seen in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.test_all_qof_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
